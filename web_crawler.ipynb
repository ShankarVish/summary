{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gain expertise in AI-driven product strategy and innovation with ISB Executive Education's Professional Certificate in Product Management Programme - The Times of India\n",
      "Edition\n",
      "IN\n",
      "IN\n",
      "US\n",
      "Sign In\n",
      "TOI\n",
      "TOI Games\n",
      "Live Videos\n",
      "City\n",
      "Metro Cities\n",
      "mumbai\n",
      "delhi\n",
      "bengaluru\n",
      "Hyderabad\n",
      "kolkata\n",
      "chennai\n",
      "Other Cities\n",
      "City\n",
      "agra\n",
      "agartala\n",
      "ahmedabad\n",
      "ajmer\n",
      "amaravati\n",
      "amritsar\n",
      "bareilly\n",
      "bhubaneswar\n",
      "bhopal\n",
      "chandigarh\n",
      "chhatrapati sambhajinagar\n",
      "coimbatore\n",
      "cuttack\n",
      "dehradun\n",
      "erode\n",
      "faridabad\n",
      "ghaziabad\n",
      "goa\n",
      "gurgaon\n",
      "guwahati\n",
      "hubballi\n",
      "imphal\n",
      "indore\n",
      "itanagar\n",
      "jaipur\n",
      "jammu\n",
      "jamshedpur\n",
      "jodhpur\n",
      "kanpur\n",
      "kochi\n",
      "kohima\n",
      "kolhapur\n",
      "kozhikode\n",
      "ludhiana\n",
      "lucknow\n",
      "madurai\n",
      "mangaluru\n",
      "meerut\n",
      "mumbai region\n",
      "mysuru\n",
      "nagpur\n",
      "nashik\n",
      "navi mumbai\n",
      "noida\n",
      "patna\n",
      "prayagraj\n",
      "puducherry\n",
      "pune\n",
      "raipur\n",
      "rajkot\n",
      "ranchi\n",
      "thane\n",
      "salem\n",
      "shillong\n",
      "shimla\n",
      "srinagar\n",
      "surat\n",
      "trichy\n",
      "thiruvananthapuram\n",
      "udaipur\n",
      "vadodara\n",
      "varanasi\n",
      "vijayawada\n",
      "visakhapatnam\n",
      "photos\n",
      "Web Stories\n",
      "India\n",
      "featured\n",
      "Times Evoke\n",
      "Cancel Patriarchy\n",
      "Maharashtra\n",
      "Delhi\n",
      "Karnataka\n",
      "Tamil Nadu\n",
      "Telangana\n",
      "Uttar Pradesh\n",
      "West Bengal\n",
      "Gujarat\n",
      "Madhya Pradesh\n",
      "Bihar\n",
      "Chandigarh\n",
      "Rajasthan\n",
      "Arunachal Pradesh\n",
      "Andhra Pradesh\n",
      "Assam\n",
      "Chhattisgarh\n",
      "Goa\n",
      "Haryana\n",
      "Himachal Pradesh\n",
      "Jammu Kashmir\n",
      "Jharkhand\n",
      "Kerala\n",
      "Manipur\n",
      "Meghalaya\n",
      "Mizoram\n",
      "Nagaland\n",
      "Odisha\n",
      "Punjab\n",
      "Sikkim\n",
      "Tripura\n",
      "Uttarakhand\n",
      "Andaman Nicobar Islands\n",
      "Dadra Nagar Haveli\n",
      "Daman Diu\n",
      "Lakshadweep\n",
      "Pondicherry\n",
      "Web Stories\n",
      "Saving Our Stripes\n",
      "Digital Arrest\n",
      "World\n",
      "Business\n",
      "Tech\n",
      "Cricket\n",
      "Sports\n",
      "Entertainment\n",
      "Astro\n",
      "TV\n",
      "Education\n",
      "Life & Style\n",
      "Web Series\n",
      "Photos\n",
      "Blogs\n",
      "Today's ePaper\n",
      "News\n",
      "Gain expertise in AI-driven product strategy and innovation with ISB Executive Education's Professional Certificate in Product Management Programme\n",
      "Gain expertise in AI-driven product strategy and innovation with ISB Executive Education's Professional Certificate in Product Management Programme\n",
      "SPOTLIGHT /\n",
      "Mar 13, 2025, 12:15 IST\n",
      "Share\n",
      "AA\n",
      "+\n",
      "Text Size\n",
      "Small\n",
      "Medium\n",
      "Large\n",
      "Product management is now a critical driver of business success, enabling organisations to deliver customer-focused solutions and optimise product lifecycles. A McKinsey article emphasises that rising customer expectations and the accelerating pace of innovation make product management increasingly vital. However, as per Gartner, product leaders face challenges such as budget restrictions and employee engagement, which can hinder strategic planning and decision-making. Managing the entire product lifecycleâfrom ideation to market deliveryâdemands expertise in customer-centric methodologies, cross-functional collaboration, and data-driven decision-making. Overcoming these challenges requires a structured learning approach tailored to this multifaceted role.\n",
      "To help aspiring product managers combine foundational knowledge with advanced skills attuned to industry demands, ISB Executive Education has designed the\n",
      "Professional Certificate in Product Management programme\n",
      ". Integrated with AI and Generative AI, the programme equips learners with the skills to effectively manage both new and existing products while leveraging the transformative potential of emerging technologies. With a focus on industry relevance and innovation, this programme offers the ISB advantage to professionals aiming to excel in the dynamic field of product management.\n",
      "An overview of the\n",
      "programme\n",
      "The Professional Certificate in Product Management by ISB Executive Education delivers a comprehensive, flexible learning experience. It includes 300+ pre-recorded videos by top ISB faculty for self-paced learning and 2 live masterclasses on Generative AI in product management. The programme features cutting-edge modules on AI and Generative AI,\n",
      "10 essential product management tools\n",
      "and 12 masterclasses by leading industry experts. Participants gain hands-on experience through 40+ case studies, quizzes, assignments, and 6+ demos and simulations, with weekly live sessions for interactive learning. Upon completion, participants earn ISB Executive Alumni status, joining an exclusive global network of senior executives and entrepreneurs.\n",
      "A key highlight of the programme is its integration of Generative AI, offering participants an in-depth understanding of its transformative role in product management. The curriculum explores how Generative AI enhances decision-making, accelerates product development, and improves user experiences. Participants will engage with various\n",
      "Generative AI models and architectures\n",
      ", such as GANs, and learn their applications in market research, product roadmaps, data analytics, and customer engagement.\n",
      "Through targeted masterclasses, participants also gain practical insights into integrating Generative AI into organisational processes. Topics include assessing organisational readiness, building a business case for AI, preparing and cleaning data, selecting suitable AI models, and integrating them with existing systems. Participants will also learn about testing, quality assurance, monitoring, and maintenance to ensure seamless AI adoption.\n",
      "Key\n",
      "programme\n",
      "outcomes\n",
      "The programme equips participants with the expertise to lead impactful product initiatives, make data-driven decisions, and excel in end-to-end product management. Graduates will:\n",
      "Adopt a strategic product mindset\n",
      ": Learn to define core product problems, map customer journeys, and create user personas to align product decisions with customer needs and business goals.\n",
      "Master product development and launches\n",
      ": Gain proficiency in crafting go-to-market strategies, driving successful product launches, and sustaining market growth.\n",
      "Optimi\n",
      "s\n",
      "e\n",
      "product roadmaps\n",
      ":\n",
      "Use data-driven approaches to evaluate and manage roadmaps, prototypes, and lifecycles, minimising risks and enhancing efficiency.\n",
      "Design scalable products\n",
      ": Create competitive, scalable products with strategies that ensure sustainable growth and strong market positioning.\n",
      "Lead cross-functional teams\n",
      ": Drive collaboration across marketing, sales, design, and engineering teams for seamless execution and organisational alignment.\n",
      "The ISB Executive Education advantage\n",
      "The Professional Certificate in Product Management by ISB Executive Education is a first-of-its-kind programme in India, and has been designed by ISB, the #1 B-School in India and the #27 B-School globally, as ranked by the FT Global MBA Ranking 2024.\n",
      "Participants benefit from ISBâs 50k+ Executive Network, offering unparalleled opportunities for networking and collaboration. The programme features renowned faculty, including Prof. Manish Gangwar, Prof. Rajendra Srivastava, and Prof. Siddharth Singh, who bring cutting-edge expertise in marketing, digital strategy, and product management, ensuring a transformative learning experience.\n",
      "ISBâs vision is to build digital leaders with an innovation and transformation mindset, equipping participants to drive 360-degree organisational growth. Graduates also join the prestigious\n",
      "ISB Executive Alumni community\n",
      ", a global network of senior executives and entrepreneurs, further enhancing professional opportunities.\n",
      "Who is this programme for?\n",
      "This programme is ideal for individuals looking to master the creation, launch, tracking, and optimisation of successful products through core product management strategies.\n",
      "Early to mid-level professionals:\n",
      "Roles such as Product Manager, Project Manager, Program Manager, Product Analyst, Marketing Operations Manager, Strategy Manager, and Customer Success Manager.\n",
      "Business heads and entrepreneurs:\n",
      "Leaders including Directors, Product Leads, Heads of Product, Managing Directors, CXOs (SMEs), and General Managers.\n",
      "Relevant industries:\n",
      "Key sectors include IT, E-commerce, Banking and Finance, FMCG, Media, Retail, and Healthcare.\n",
      "A look at the key\n",
      "programme\n",
      "details\n",
      "Eligibility:\n",
      "Any graduate/diploma holder\n",
      "Programme\n",
      "start date:\n",
      "31 March 2025\n",
      "Duration:\n",
      "28 weeks (4â6 hours per week)\n",
      "About ISB Executive Education\n",
      "ISB Executive Education empowers participants with the skills, mindsets, and networks required to manage and lead in this evolving landscape, enabling them to achieve their distinct personal and professional goals. Recognised as #1 in India for the third consecutive year and #26 globally in the Financial Times (FT) Executive Education Custom Rankings 2024 and #3 in India, #65 globally in the FT Executive Education Open Ranking 2023, ISB Executive Education is committed to prepare working professionals to excel in the new global business environment. This is achieved through fostering engaging exchanges between renowned industry leaders and academia and drawing globally renowned faculty from the world's top business schools. Through meticulously designed programmes, participants gain from both advanced management research and the vast experiences of their peers, ensuring they are well-equipped to navigate the complexities of the modern business landscape.\n",
      "About Emeritus\n",
      "ISB Executive Education is collaborating with an online education provider, Emeritus, to offer a portfolio of high-impact online programmes. Working with Emeritus gives ISB Executive Education the advantage of broadening its access beyond their on-campus offerings in a collaborative and engaging format that stays true to the quality of ISB Executive Education. Emeritusâ approach to learning is built on a cohort-based design to maximise peer-to-peer sharing and includes video lectures with world-class faculty and hands-on project-based learning. More than 300,000 students from over 200 countries have benefitted professionally from Emeritusâ programmes.\n",
      "Disclaimer: This article has been produced on behalf of Erulearning by Times Internetâs Spotlight team.\n",
      "References\n",
      "1. https://www.mckinsey.com/capabilities/operations/our-insights/turn-product-managers-into-super-conductors\n",
      "2. https://www.gartner.com/peer-community/oneminuteinsights/omi-2024-product-management-priorities-challenges-insights-field-3d3\n",
      "End of Article\n",
      "FOLLOW US ON SOCIAL MEDIA\n",
      "TOP TRENDING\n",
      "TASMAC Scam Protest Chennai\n",
      "Prince Harry\n",
      "Vadodara Car Crash\n",
      "Butch Wilmores\n",
      "Astronauts Sunita Williams\n",
      "IPL Full Schedule\n",
      "PM Modi\n",
      "Raphael Glucksmann\n",
      "Wayne Gretzkys\n",
      "PM Modi Podcast\n",
      "Trending Stories\n",
      "In Section\n",
      "Entire Website\n",
      "For the first time in history, Delhi Police to appoint SHOs through merit-based exam\n",
      "'Reflects pragmatic approach': China mouthpiece on PM Modi's remarks during Lex Fridman podcast\n",
      "India tells US to crackdown on anti-India activities of Khalistani terror outfit SFJ: Report\n",
      "'Language not for hating': CM Naidu mediates after deputy CM Pawan Kalyan's remark amid NEP row\n",
      "SSC CGL 2024 final answer key: Amid allegations on normalisation process, candidates demand transparency and release of answer key\n",
      "Former union minister Debendra Pradhan passes away at 83\n",
      "Will Bitcoin price crash to $20,000? Peter Schiff makes big prediction if Nasdaq enters bear market phase\n",
      "'Accused looked intoxicated, was overspeeding for enjoyment': Survivor recalls Vadodara car crash horror\n",
      "'Did it as he abused Islam': Pakistani don Shehzab Bhatti releases video of Jalandhar grenade attack\n",
      "'Catastrophic consequences for city': Why Mumbai residents are up against Rs 229 crore jetty near Gateway of India\n",
      "Violent clashes erupt in Nagpur: Vehicles torched, cops injured; CM Fadnavis makes appeal\n",
      "'Thank you my friend': PM joins Truth Social after Trump's Fridman post\n",
      "How Ranjani Srinivasan dodged US immigration and fled to Canada\n",
      "Is Nitish Kumar preparing for 'son'rise in Bihar?\n",
      "In 7 charts: What Indian-Americans think of Trump, PM Modi\n",
      "How to deal with Trump: Flattery, retaliation, or holding out?\n",
      "Anti-DEI Trump may fuel rise in hate speech here\n",
      "Elon Musk's ex on hardest part of parenting: I remember being a horror about...\n",
      "'He has got everything': Kohli backs Patidar to lead RCB for years\n",
      "Gold at record high! Check 5 charts before putting money in gold\n",
      "Explore Every Corner\n",
      "Across The Globe\n",
      "Stefon Diggs\n",
      "Aaron Rodgers\n",
      "Sunita Williams\n",
      "Donna Kelce\n",
      "Kylie Kelce\n",
      "NBA offseason Rumors\n",
      "Wayne Gretzkys Wife\n",
      "Prince Harry\n",
      "WWE Monday Night Raw Preview\n",
      "Surprising Returns at WWE Wrestlemania\n",
      "Donald Trump\n",
      "Conor Mcgregor\n",
      "Columbia Student Ranjani Srinivasan\n",
      "Manikarnika Dutta\n",
      "Trump immigration policy\n",
      "Carmelo Anthony\n",
      "Fernando Tatis Jr Net Worth\n",
      "Liv Morgan\n",
      "Ramakanta Rath\n",
      "GTA 6 Delayed to 2026\n",
      "Baloch Rebels Attack Footage\n",
      "Air Canada Israel error\n",
      "Green Card Holders\n",
      "Vadodara rash driving case\n",
      "Taylor Swift\n",
      "Bengaluru Property Owner Fine\n",
      "NFL Rumors\n",
      "Public Holidays in March\n",
      "Priest Suicide Ahmedabad\n",
      "Abu Qatal\n",
      "About us\n",
      "Create Your Own Ad\n",
      "Terms of Use and Grievance Redressal Policy\n",
      "Privacy policy\n",
      "Advertise with us\n",
      "RSS\n",
      "Newsletter\n",
      "Feedback\n",
      "ePaper\n",
      "Sitemap\n",
      "Archives\n",
      "Living and Entertainment\n",
      "Cricbuzz\n",
      "Lifestyle\n",
      "Newspaper Subscription\n",
      "Food News\n",
      "TV\n",
      "Times Prime\n",
      "Travel Destinations\n",
      "Whats Hot\n",
      "ETimes\n",
      "Times Life\n",
      "Times Pets\n",
      "MyLifeXP\n",
      "Services\n",
      "CouponDunia\n",
      "TechGig\n",
      "TimesJobs\n",
      "Bollywood News\n",
      "Times Mobile\n",
      "Gadgets\n",
      "Work with Us\n",
      "Colombia\n",
      "Hot on the Web\n",
      "Bahadur Shah Zafar\n",
      "Salman Khan\n",
      "Sania Mirza\n",
      "Belly Fat Burning Tips\n",
      "Sunita Williams\n",
      "Aamir Khan\n",
      "Yellowstone National Park\n",
      "Prince William\n",
      "Optical illusion\n",
      "Orry\n",
      "Personality Test\n",
      "Sunita Williams\n",
      "Hina Khan\n",
      "Munawar Faruqui\n",
      "Gut Health Test\n",
      "Bharti Singh\n",
      "Kalpana Chawla\n",
      "Celebrity MasterChef\n",
      "Abhishek Bachchan\n",
      "0ptical Illusion\n",
      "Traditional Mehendi Designs\n",
      "Viral Kohli\n",
      "Chhaava Box Office Collection\n",
      "Sweet Potato Paratha Recipe\n",
      "The Diplomat Collection\n",
      "Unique Animals\n",
      "Bindu Ghosh\n",
      "National Parks Asia\n",
      "Freshwater Aquarium Fishes\n",
      "J Letter Countries\n",
      "Trending Topics\n",
      "Khushdil Shah\n",
      "Rohit Sharma\n",
      "NZ Champions Trophy Defeat\n",
      "CM Fadnavis\n",
      "Lex Fridman Podcast\n",
      "Worlds Longest Hyperloop Tube\n",
      "Scarlett Johansson\n",
      "Gold Prices Record\n",
      "IPL 2025 Free Streaming\n",
      "TSPSC Group 3 Result\n",
      "MS Dhoni\n",
      "CDAC AFCAT Result\n",
      "Bihar Board Class 12 Result\n",
      "Faf Du Plessis\n",
      "SSC CGL Final Answer Key Protest\n",
      "Shantanu Naidu\n",
      "RSMSSB Stenographer Phase 2 Admit Card\n",
      "IPL 2025 Player Replacement Rules\n",
      "TG PGECET 2025 Registration\n",
      "RCB Unbox Event 2025\n",
      "PM Modi\n",
      "NYT Strands\n",
      "FBI Warning\n",
      "iPhone 17 AIR\n",
      "NYT Connections\n",
      "Wordle Today\n",
      "Sunita Williams\n",
      "Elon Musk\n",
      "Live Cricket Score\n",
      "IPL Schedule 2025\n",
      "Popular Categories\n",
      "Headlines\n",
      "Sports News\n",
      "Business News\n",
      "India News\n",
      "World News\n",
      "Bollywood News\n",
      "Health+ Tips\n",
      "Indian TV Shows\n",
      "Technology\n",
      "IPL\n",
      "Travel\n",
      "Etimes\n",
      "Health & Fitness\n",
      "WWE\n",
      "NFL\n",
      "ICC Champions Trophy\n",
      "Astrology\n",
      "Weather Today\n",
      "How to watch IPL 2025\n",
      "How to watch IPL in Canada\n",
      "How to watch IPL in USA\n",
      "Stock Market Holidays\n",
      "NSE Holidays\n",
      "BSE Holidays\n",
      "MCX Holidays\n",
      "Settlement Holidays List 2025\n",
      "Technology News\n",
      "International Sports\n",
      "Public Holidays\n",
      "Bank Holidays\n",
      "Latest News\n",
      "NHL Weekly: Jesper Bratt Shines, Brandon Montour Makes History, & Jack Eichel Sets a New Record\n",
      "Jean Silvaâs Chest Tattoo: Unlocking the Meaning Behind His Lock Tattoo\n",
      "Kim Sharma shares how she made Orry one of the most successful âsocial experimentsâ: 'He is not an influencer...'\n",
      "Toronto Maple Leafs vs Calgary Flames: Injury report, where to watch, stats, predictions, betting odds, top players, and more\n",
      "Dana Whiteâs UFC Prospect Thomas Carty Loses Fight Against 409-lb American Opponent Dajuan Calloway\n",
      "IPL 2025: Chennai Super Kings Team Preview - SWOT Analysis, Past Performances and Best XI\n",
      "Watch: Violent clashes erupt in Nagpur, vehicles torched; cops deployed\n",
      "'Iran to suffer dire consequences': Trump warns Tehran against backing Houthis\n",
      "IPL 2025: Kolkata Knight Riders Team Preview â SWOT Analysis and Best XI\n",
      "Madhuri Dixit says Rasha Thadani would be perfect for 'Ek Do Teen' remake: 'Her dance is very graceful'\n",
      "Two robbery suspects arrested after shootout in Fatehgarh Sahib; police officer injured\n",
      "â Dodgers concerned as Mookie Betts shows fatigue ahead of opening game\n",
      "Hilaria Baldwin addresses backlash over her accent controversy\n",
      "12 Patiala cops suspended for allegedly assaulting army officer and son; inquiry to conclude in 45 days\n",
      "Can moving abroad make you feel more Indian? A viral social media debate\n",
      "John Cena's Cryptic Post Sparks Speculations Ahead of WWE Raw Return\n",
      "Marvel Rivals Invitational 2025: Highlights and Best Plays from North America Swiss Stage Tournament\n",
      "SSC Stenographer 2024 skill test dates announced: Check details and vacancy distribution\n",
      "Copyright Â© 2025 Bennett, Coleman & Co. Ltd. All rights reserved. For reprint rights: Times Syndication Service\n",
      "FOLLOW US ON\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def web_crawler(url):\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}  # Mimic a real browser\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raise an error for HTTP errors\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract all text content from the webpage\n",
    "        jhakas = soup.get_text(separator='\\n', strip=True)\n",
    "        \n",
    "        return jhakas\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    url = input(\"Enter URL: \")\n",
    "    jhakas = web_crawler(url)\n",
    "    print(jhakas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_words': 6, 'unique_words': 6, 'common_words': [('paste', 1), ('the', 1), ('extracted', 1), ('webpage', 1), ('content', 1), ('here', 1)], 'topics': []}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def analyze_content(content):\n",
    "    # Convert content to lowercase\n",
    "    content = content.lower()\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    words = re.findall(r'\\b[a-z]{3,}\\b', content)\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_freq = Counter(words)\n",
    "    \n",
    "    # Get the most common words\n",
    "    common_words = word_freq.most_common(10)\n",
    "    \n",
    "    # Count total words and unique words\n",
    "    total_words = len(words)\n",
    "    unique_words = len(word_freq)\n",
    "    \n",
    "    # Detect potential topics by looking at frequent words\n",
    "    topics = [word for word, count in common_words if count > 3]\n",
    "    \n",
    "    analysis = {\n",
    "        \"total_words\": total_words,\n",
    "        \"unique_words\": unique_words,\n",
    "        \"common_words\": common_words,\n",
    "        \"topics\": topics\n",
    "    }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    jhakas = \"\"\"Paste the extracted webpage content here.\"\"\"\n",
    "    analysis_result = analyze_content(jhakas)\n",
    "    print(analysis_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shankarvishnu/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4935be7e45bb4686b20758b1d0ac4957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c46b53f874491b9db862e27c1cd12c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shankarvishnu/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:479: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8f90329ebc42ff93c4a79c1fa88352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5641a58797b4c39bdad8dd258b6d1c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4767fc58304b3f91b4cd7690831eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 150, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paste the extracted webpage content here . Paste it into the page you want to see if you like to see the content . Paste the content here. Paste it in the next copy of the page to see how it looks like it would look like it was taken .\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def get_summary(content, max_length=150, min_length=50):\n",
    "    try:\n",
    "        # Load a pre-trained summarization pipeline\n",
    "        summarizer = pipeline(\"summarization\")\n",
    "        summary = summarizer(content, max_length=max_length, min_length=min_length, do_sample=False)\n",
    "        return summary[0]['summary_text'] if summary else \"Summary could not be generated.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    jhakas = \"\"\"Paste the extracted webpage content here.\"\"\"\n",
    "    summary = get_summary(jhakas)\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Content:\n",
      " Nasa astronauts Sunita Williams, Butch Wilmore head to Earth: Why their 8-day mission turned into 9 months in space - The Times of India\n",
      "Edition\n",
      "IN\n",
      "IN\n",
      "US\n",
      "English\n",
      "English\n",
      "à¤¹à¤¿à¤¨à¥à¤¦à¥\n",
      "à¤®à¤°à¤¾à¤ à¥\n",
      "à²à²¨à³à²¨à²¡\n",
      "à®¤à®®à®¿à®´à¯\n",
      "à¦¬à¦¾à¦à¦²à¦¾\n",
      "à´®à´²à´¯à´¾à´³à´\n",
      "à°¤à±à°²à±à°à±\n",
      "àªà«àªàª°àª¾àª¤à«\n",
      "Sign In\n",
      "TOI\n",
      "Science\n",
      "Today's ePaper\n",
      "News\n",
      "Science News\n",
      "Nasa astronauts Sunita Williams, Butch Wilmore head to Earth: Why their 8-day mission turned into 9 months in space\n",
      "Nasa astronauts Sunita Williams, Butch Wilmore head to Earth: Why their 8-day mission turned into 9 months in space\n",
      "TIMESOFINDIA.COM /\n",
      "Updated: Mar 18, 2025, 21:41 IST\n",
      "Share\n",
      "AA\n",
      "+\n",
      "Text Size\n",
      "Small\n",
      "Medium\n",
      "Large\n",
      "Butch Wilmore and Sunita Williams on their way to Earth\n",
      "Nasa astronauts\n",
      "Sunita Williams\n",
      "and\n",
      "Butch Wilmore\n",
      "are finally on their way back to Earth via\n",
      "SpaceX\n",
      ", concluding an extraordinary extended mission spanning more than nine months.\n",
      "The vessel detached during the early hours, targeting a water landing near  ...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3288 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      " Error: index out of range in self\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "\n",
    "def web_crawler(url):\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}  # Mimic a real browser\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raise an error for HTTP errors\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract all text content from the webpage\n",
    "        jhakas = soup.get_text(separator='\\n', strip=True)\n",
    "        \n",
    "        return jhakas\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def get_summary(content, max_length=150, min_length=50):\n",
    "    try:\n",
    "        # Load a pre-trained summarization pipeline\n",
    "        summarizer = pipeline(\"summarization\")\n",
    "        summary = summarizer(content, max_length=max_length, min_length=min_length, do_sample=False)\n",
    "        return summary[0]['summary_text'] if summary else \"Summary could not be generated.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    url = input(\"Enter URL: \")\n",
    "    jhakas = web_crawler(url)\n",
    "    print(\"Extracted Content:\\n\", jhakas[:1000], \"...\\n\")  # Print a snippet of the extracted content\n",
    "    \n",
    "    summary = get_summary(jhakas)\n",
    "    print(\"\\nSummary:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Content:\n",
      " Nasa astronauts Sunita Williams, Butch Wilmore head to Earth: Why their 8-day mission turned into 9 months in space - The Times of India\n",
      "Edition\n",
      "IN\n",
      "IN\n",
      "US\n",
      "English\n",
      "English\n",
      "à¤¹à¤¿à¤¨à¥à¤¦à¥\n",
      "à¤®à¤°à¤¾à¤ à¥\n",
      "à²à²¨à³à²¨à²¡\n",
      "à®¤à®®à®¿à®´à¯\n",
      "à¦¬à¦¾à¦à¦²à¦¾\n",
      "à´®à´²à´¯à´¾à´³à´\n",
      "à°¤à±à°²à±à°à±\n",
      "àªà«àªàª°àª¾àª¤à«\n",
      "Sign In\n",
      "TOI\n",
      "Science\n",
      "Today's ePaper\n",
      "News\n",
      "Science News\n",
      "Nasa astronauts Sunita Williams, Butch Wilmore head to Earth: Why their 8-day mission turned into 9 months in space\n",
      "Nasa astronauts Sunita Williams, Butch Wilmore head to Earth: Why their 8-day mission turned into 9 months in space\n",
      "TIMESOFINDIA.COM /\n",
      "Updated: Mar 18, 2025, 21:41 IST\n",
      "Share\n",
      "AA\n",
      "+\n",
      "Text Size\n",
      "Small\n",
      "Medium\n",
      "Large\n",
      "Butch Wilmore and Sunita Williams on their way to Earth\n",
      "Nasa astronauts\n",
      "Sunita Williams\n",
      "and\n",
      "Butch Wilmore\n",
      "are finally on their way back to Earth via\n",
      "SpaceX\n",
      ", concluding an extraordinary extended mission spanning more than nine months.\n",
      "The vessel detached during the early hours, targeting a water landing near  ...\n",
      "\n",
      "Summary:\n",
      " Nasa astronauts Sunita Williams, Butch Wilmore head to Earth: Why their 8-day mission turned into 9 months in space - The Times of India Edition. The extended mission resulted from technical issues with their original return vehicle, Boeing's Starliner. The Crew Dragon spacecraft is scheduled to splash down near Florida's Gulf Coast at 5:57 pm EDT. Recovery teams will retrieve the vessel and assist the astronauts in disembarking onto a ship. The four crew members will subsequently be transported by air to Houston, where Nasa's Johnson Space Center serves as primary centre for human spaceflight operations. BSE Sensex rallies over 1,200 points; Nifty50 above 22,850 - top 8 reasons for D-Street party. DDA offers 828 LIG, EWS flats; booking starts today. Sunita Williams departs ISS for Earth: How and where she will land, when and where to watch Parliament Budget Session. IPL 2025: Punjab Kings Team Preview - SWOT Analysis and Best XI Pushpa Impossible: Urvashi Dholakia is back as Devi Singh Shekhawat. Ayesha Jhulka reveals she felt Divya Bharti's presence at Rang screening post her demise: 'I couldn't sleep for a long time...' How to play MLB The Show 25 with friends on PS5, Xbox and Nintendo Switch?\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "\n",
    "def web_crawler(url):\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}  # Mimic a real browser\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raise an error for HTTP errors\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract all text content from the webpage\n",
    "        jhakas = soup.get_text(separator='\\n', strip=True)\n",
    "        \n",
    "        # Preprocess extracted content to remove excessive whitespace\n",
    "        jhakas = '\\n'.join([line.strip() for line in jhakas.splitlines() if line.strip()])\n",
    "        \n",
    "        return jhakas\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def split_text(text, max_words=500):\n",
    "    \"\"\"Splits text into chunks of max_words length to fit within the summarization model's limit.\"\"\"\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i + max_words]) for i in range(0, len(words), max_words)]\n",
    "\n",
    "def get_summary(content, max_length=150, min_length=50):\n",
    "    try:\n",
    "        word_count = len(content.split())\n",
    "        \n",
    "        if word_count < 50:\n",
    "            return \"Content too short for summarization.\"\n",
    "        \n",
    "        summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")  # Specify model explicitly\n",
    "\n",
    "        if word_count > 1024:\n",
    "            chunks = split_text(content, max_words=500)  # Split into smaller parts\n",
    "            summaries = [summarizer(chunk, max_length=max_length, min_length=min_length, do_sample=False)[0]['summary_text'] for chunk in chunks]\n",
    "            return \" \".join(summaries)  # Combine partial summaries\n",
    "        else:\n",
    "            summary = summarizer(content, max_length=max_length, min_length=min_length, do_sample=False)\n",
    "            return summary[0]['summary_text'] if summary else \"Summary could not be generated.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    url = input(\"Enter URL: \")\n",
    "    jhakas = web_crawler(url)\n",
    "    print(\"\\nExtracted Content:\\n\", jhakas[:1000], \"...\")  # Show snippet\n",
    "    \n",
    "    summary = get_summary(jhakas)\n",
    "    print(\"\\nSummary:\\n\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Content:\n",
      " What Is Machine Learning (ML)? | IBM\n",
      "What is machine learning?\n",
      "Artificial Intelligence\n",
      "Link copied\n",
      "What is machine learning?\n",
      "Machine learning (ML) is a branch of\n",
      "artificial intelligence (AI)\n",
      "focused on enabling computers and machines to imitate the way that humans learn, to perform tasks autonomously, and to improve their performance and accuracy through experience and exposure to more data.\n",
      "UC Berkeley\n",
      "breaks out the learning system of a machine learning algorithm into three main parts.\n",
      "A Decision Process: In general, machine learning algorithms are used to make a prediction or classification. Based on some input data, which can be labeled or unlabeled, your algorithm will produce an estimate about a pattern in the data.\n",
      "An Error Function: An error function evaluates the prediction of the model. If there are known examples, an error function can make a comparison to assess the accuracy of the model.\n",
      "A Model Optimization Process: If the model can fit better to the data points in the tr ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 150, but your input_length is only 123. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      " Machine learning is a branch of artificial intelligence focused on enabling computers and machines to imitate the way that humans learn. UC Berkeley breaks out the learning system of a machine learning algorithm into three main parts. \"Deep\" machine learning can use labeled datasets, also known as supervised learning, to inform its algorithm.\n",
      "The “deep” in deep learning is just referring to the number of layers in a neural network. Each node, or artificial neuron, connects to another and has an associated weight and threshold. A neural network that consists of more than three layers can be considered a deep learning algorithm.\n",
      "A number of machine learning algorithms are commonly used. Depending on your budget, need for speed and precision required, each algorithm type has its own advantages and disadvantages. For a deep dive into the differences between these approaches, check out \" Supervised vs. Unsupervised Learning: What's the Difference? \"\n",
      "There are many advantages to machine learning that businesses can leverage for new efficiencies. Machine learning identifies patterns and trends in massive volumes of data that humans might not spot at all. On the downside, machine learning requires large training datasets that are accurate and unbiased.\n",
      "Artificial intelligence has raised a number of ethical concerns about the technology. Many researchers are not concerned with the idea of AI surpassing human intelligence in the near future. The biggest challenge with artificial intelligence and its effect on the job market will be helping people to transition to new roles that are in demand.\n",
      "Bias and discrimination have raised many ethical questions regarding the use of artificial intelligence. How can we safeguard against bias and discrimination when the training data itself may be generated by biased human processes? Amazon unintentionally discriminated against job candidates by gender for technical roles, and the company ultimately had to scrap the project.\n",
      "Learn how to confidently incorporate generative AI and machine learning into your business. Discover watsonx.ai Artificial intelligence solutions. Reinvent critical workflows and operations by adding AI to maximize experiences, real-time decision-making and business value.\n",
      "Get one-stop access to capabilities that span the AI development lifecycle. Produce powerful AI solutions with user-friendly interfaces, workflows and access to industry-standard APIs and SDKs. Explore watsonx.ai Book a live demo\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "\n",
    "def web_crawler(url):\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}  # Mimic a real browser\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raise an error for HTTP errors\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract all text content from the webpage\n",
    "        extracted_text = soup.get_text(separator='\\n', strip=True)\n",
    "        \n",
    "        # Preprocess extracted content to remove excessive whitespace\n",
    "        extracted_text = '\\n'.join([line.strip() for line in extracted_text.splitlines() if line.strip()])\n",
    "        \n",
    "        return extracted_text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def split_text(text, max_words=500):\n",
    "    \"\"\"Splits text into chunks of max_words length to fit within the summarization model's limit.\"\"\"\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i + max_words]) for i in range(0, len(words), max_words)]\n",
    "\n",
    "def get_summary(content, max_length=150, min_length=50):\n",
    "    try:\n",
    "        word_count = len(content.split())\n",
    "\n",
    "        if word_count < 50:\n",
    "            return \"Content too short for summarization.\"\n",
    "\n",
    "        summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")  # Specify model explicitly\n",
    "\n",
    "        if word_count > 1024:\n",
    "            chunks = split_text(content, max_words=500)  # Split into smaller parts\n",
    "            summaries = [summarizer(chunk, max_length=max_length, min_length=min_length, do_sample=False)[0]['summary_text'] for chunk in chunks]\n",
    "            return \"\\n\".join(summaries)  # Ensure each summary appears on a new line\n",
    "        else:\n",
    "            summary = summarizer(content, max_length=max_length, min_length=min_length, do_sample=False)\n",
    "            return summary[0]['summary_text'] if summary else \"Summary could not be generated.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    url = input(\"Enter URL: \")\n",
    "    extracted_text = web_crawler(url)\n",
    "    print(\"\\nExtracted Content:\\n\", extracted_text[:1000], \"...\")  # Show snippet\n",
    "    \n",
    "    summary = get_summary(extracted_text)\n",
    "    print(\"\\nSummary:\\n\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEEPSEEK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Metadata ===\n",
      "Title: Nasa astronauts Sunita Williams, Butch Wilmore head to Earth: Why their 8-day mission turned into 9 months in space - The Times of India\n",
      "Author: \n",
      "Language: en\n",
      "\n",
      "=== Content Analysis ===\n",
      "Word Count: 692\n",
      "Unique Words: 330\n",
      "Readability Level: Intermediate\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import readability\n",
    "from bs4.element import SoupStrainer\n",
    "import re\n",
    "\n",
    "class WebContentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'\n",
    "        }\n",
    "        \n",
    "    def _is_valid_url(self, url):\n",
    "        try:\n",
    "            result = urlparse(url)\n",
    "            return all([result.scheme, result.netloc])\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def _get_webpage(self, url):\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                url,\n",
    "                headers=self.headers,\n",
    "                timeout=10,\n",
    "                allow_redirects=True,\n",
    "                verify=True\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            if 'text/html' not in response.headers.get('Content-Type', ''):\n",
    "                raise ValueError(\"URL does not point to HTML content\")\n",
    "                \n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to retrieve webpage: {str(e)}\")\n",
    "\n",
    "    def _extract_metadata(self, soup):\n",
    "        metadata = {\n",
    "            'title': '',\n",
    "            'author': '',\n",
    "            'description': '',\n",
    "            'keywords': [],\n",
    "            'language': 'en'\n",
    "        }\n",
    "        \n",
    "        # Title\n",
    "        title_tag = soup.find('title')\n",
    "        if title_tag:\n",
    "            metadata['title'] = title_tag.get_text().strip()\n",
    "            \n",
    "        # Meta tags\n",
    "        for meta in soup.find_all('meta'):\n",
    "            if 'name' in meta.attrs:\n",
    "                name = meta.attrs['name'].lower()\n",
    "                if name == 'description':\n",
    "                    metadata['description'] = meta.attrs.get('content', '')\n",
    "                elif name == 'author':\n",
    "                    metadata['author'] = meta.attrs.get('content', '')\n",
    "                elif name == 'keywords':\n",
    "                    keywords = meta.attrs.get('content', '')\n",
    "                    metadata['keywords'] = [k.strip() for k in keywords.split(',')]\n",
    "                    \n",
    "        # Language detection\n",
    "        lang_tag = soup.find('html')\n",
    "        if lang_tag and 'lang' in lang_tag.attrs:\n",
    "            metadata['language'] = lang_tag.attrs['lang']\n",
    "            \n",
    "        return metadata\n",
    "\n",
    "    def _clean_content(self, text):\n",
    "        # Remove excessive whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Remove non-printable characters\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def _extract_main_content(self, html):\n",
    "        try:\n",
    "            doc = readability.Document(html)\n",
    "            content = doc.summary()\n",
    "            soup = BeautifulSoup(content, 'lxml', parse_only=SoupStrainer(['p', 'h1', 'h2', 'h3']))\n",
    "            return self._clean_content(soup.get_text())\n",
    "        except Exception as e:\n",
    "            # Fallback method\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "            for element in soup(['header', 'footer', 'nav', 'aside', 'script', 'style', 'noscript', 'meta', 'link']):\n",
    "                element.decompose()\n",
    "            return self._clean_content(soup.get_text())\n",
    "\n",
    "    def _analyze_content(self, text):\n",
    "        words = re.findall(r'\\w+', text.lower())\n",
    "        word_count = len(words)\n",
    "        unique_words = len(set(words))\n",
    "        sentence_count = len(re.findall(r'[.!?]+', text))\n",
    "        \n",
    "        return {\n",
    "            'word_count': word_count,\n",
    "            'unique_words': unique_words,\n",
    "            'sentence_count': sentence_count,\n",
    "            'readability_level': \"Basic\" if word_count < 500 else \"Intermediate\" if word_count < 1500 else \"Advanced\"\n",
    "        }\n",
    "\n",
    "    def process_url(self, url):\n",
    "        if not self._is_valid_url(url):\n",
    "            return {'error': 'Invalid URL format'}\n",
    "            \n",
    "        try:\n",
    "            html = self._get_webpage(url)\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "            \n",
    "            result = {\n",
    "                'metadata': self._extract_metadata(soup),\n",
    "                'content': self._extract_main_content(html),\n",
    "                'analysis': None\n",
    "            }\n",
    "            \n",
    "            if result['content']:\n",
    "                result['analysis'] = self._analyze_content(result['content'])\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'error': str(e)}\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = WebContentAnalyzer()\n",
    "    url = input(\"Enter a URL to analyze: \")\n",
    "    \n",
    "    result = analyzer.process_url(url)\n",
    "    \n",
    "    if 'error' in result:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "    else:\n",
    "        print(\"\\n=== Metadata ===\")\n",
    "        print(f\"Title: {result['metadata']['title']}\")\n",
    "        print(f\"Author: {result['metadata']['author']}\")\n",
    "        print(f\"Language: {result['metadata']['language']}\")\n",
    "        \n",
    "        print(\"\\n=== Content Analysis ===\")\n",
    "        print(f\"Word Count: {result['analysis']['word_count']}\")\n",
    "        print(f\"Unique Words: {result['analysis']['unique_words']}\")\n",
    "        print(f\"Readability Level: {result['analysis']['readability_level']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text:\n",
      "\n",
      "Nasa astronauts Sunita Williams and Butch Wilmore are finally on their way back to Earth via SpaceX , concluding an extraordinary extended mission spanning more than nine months. The vessel detached during the early hours, targeting a water landing near the Florida coastline by early evening, subject to weather conditions. Here's how a journey that was set for 8 days took 9 months What was initially planned as an eight-day mission transformed into a nine-month stay. The original schedule included a June 5 launch using Boeing's Starliner crew capsule. Technical complications during the space station approach led Nasa to return Starliner empty and transfer the test pilots to SpaceX. Further technical issues with the SpaceX capsule caused an additional month's delay. June 2024: Starliner launched for the ISS on June 5, scheduled to return by June 14. However, helium leaks led to multiple delays, pushing the return date first to June 18, then June 26, and beyond. August 2024: Nasa suggeste...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0t/rr73mn4s3zx_tw23p5ls40w00000gn/T/ipykernel_2474/1777010907.py:51: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  texts = soup.findAll(text=True)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import readability\n",
    "import re\n",
    "\n",
    "def extract_web_text(url):\n",
    "    \"\"\"Extract and clean main text content from a webpage\"\"\"\n",
    "    \n",
    "    # Validate URL format\n",
    "    try:\n",
    "        result = urlparse(url)\n",
    "        if not all([result.scheme, result.netloc]):\n",
    "            return {'error': 'Invalid URL format'}\n",
    "    except:\n",
    "        return {'error': 'Invalid URL format'}\n",
    "\n",
    "    # Configure headers and fetch content\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10, verify=True)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Check for HTML content\n",
    "        if 'text/html' not in response.headers.get('Content-Type', ''):\n",
    "            return {'error': 'Non-HTML content'}\n",
    "            \n",
    "        html = response.content\n",
    "    except Exception as e:\n",
    "        return {'error': f'Failed to retrieve page: {str(e)}'}\n",
    "\n",
    "    # Extract main content using readability\n",
    "    try:\n",
    "        doc = readability.Document(html)\n",
    "        content_html = doc.summary()\n",
    "    except:\n",
    "        content_html = html  # Fallback to raw HTML\n",
    "\n",
    "    # Clean extracted text\n",
    "    soup = BeautifulSoup(content_html, 'html.parser')\n",
    "    \n",
    "    # Remove unwanted elements\n",
    "    for element in soup(['header', 'footer', 'nav', 'aside', 'script', 'style', \n",
    "                       'noscript', 'meta', 'link', 'button', 'form', 'comment']):\n",
    "        element.decompose()\n",
    "\n",
    "    # Get visible text\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_text = filter(lambda t: t.parent.name not in ['pre', 'code'], texts)\n",
    "    visible_text = filter(lambda x: not isinstance(x, Comment), visible_text)\n",
    "    cleaned_text = ' '.join(t.strip() for t in visible_text if t.strip())\n",
    "\n",
    "    # Final cleaning\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # Collapse whitespace\n",
    "    cleaned_text = re.sub(r'\\[.*?\\]', '', cleaned_text)  # Remove brackets content\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "\n",
    "    return {'text': cleaned_text} if cleaned_text else {'error': 'No text content found'}\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    url = input(\"Enter URL: \")\n",
    "    result = extract_web_text(url)\n",
    "    \n",
    "    if 'error' in result:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "    else:\n",
    "        print(\"Extracted Text:\\n\")\n",
    "        print(result['text'][:1000] + \"...\")  # Print first 1000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Extracted Text ===\n",
      "\n",
      "Nasa astronauts Sunita Williams and Butch Wilmore are finally on their way back to Earth via SpaceX , concluding an extraordinary extended mission spanning more than nine months. The vessel detached during the early hours, targeting a water landing near the Florida coastline by early evening, subject to weather conditions. Here's how a journey that was set for 8 days took 9 months What was initially planned as an eight-day mission transformed into a nine-month stay. The original schedule included a June 5 launch using Boeing's Starliner crew capsule. Technical complications during the space station approach led Nasa to return Starliner empty and transfer the test pilots to SpaceX. Further technical issues with the SpaceX capsule caused an additional month's delay. June 2024: Starliner launched for the ISS on June 5, scheduled to return by June 14. However, helium leaks led to multiple delays, pushing the return date first to June 18, then June 26, and beyond. August 2024: Nasa suggeste...\n",
      "\n",
      "=== Analysis ===\n",
      "Title: Nasa astronauts Sunita Williams, Butch Wilmore head to Earth: Why their 8-day mission turned into 9 months in space - The Times of India\n",
      "Description: Science News: Nasa astronauts Sunita Williams and Butch Wilmore are finally on their way back to Earth via SpaceX, concluding an extraordinary extended mission span.\n",
      "Word Count: 735\n",
      "Sentence Count: 47\n",
      "\n",
      "Readability Scores:\n",
      "Average Word Length: 5.21\n",
      "Average Sentence Length: 15.64\n",
      "Unique Word Ratio: 0.463\n",
      "\n",
      "Top Keywords:\n",
      "- Nasa (14 occurrences)\n",
      "- Astronauts (13 occurrences)\n",
      "- Crew (9 occurrences)\n",
      "- Space (9 occurrences)\n",
      "- Return (9 occurrences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0t/rr73mn4s3zx_tw23p5ls40w00000gn/T/ipykernel_2474/2775560322.py:52: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  texts = soup.findAll(text=True)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import readability\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def extract_and_analyze(url):\n",
    "    \"\"\"Extract text content and perform analysis from a webpage\"\"\"\n",
    "    \n",
    "    result = {'text': '', 'analysis': {}, 'error': None}\n",
    "    \n",
    "    # Validate URL\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        if not all([parsed.scheme, parsed.netloc]):\n",
    "            result['error'] = \"Invalid URL format\"\n",
    "            return result\n",
    "    except:\n",
    "        result['error'] = \"Invalid URL format\"\n",
    "        return result\n",
    "\n",
    "    # Fetch content\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10, verify=True)\n",
    "        response.raise_for_status()\n",
    "        if 'text/html' not in response.headers.get('Content-Type', ''):\n",
    "            result['error'] = \"Non-HTML content\"\n",
    "            return result\n",
    "        html = response.content\n",
    "    except Exception as e:\n",
    "        result['error'] = f\"Connection error: {str(e)}\"\n",
    "        return result\n",
    "\n",
    "    # Extract main content\n",
    "    try:\n",
    "        doc = readability.Document(html)\n",
    "        content_html = doc.summary()\n",
    "    except:\n",
    "        content_html = html\n",
    "\n",
    "    # Clean text\n",
    "    soup = BeautifulSoup(content_html, 'html.parser')\n",
    "    \n",
    "    # Remove unwanted elements\n",
    "    for element in soup(['header', 'footer', 'nav', 'aside', 'script', 'style',\n",
    "                       'noscript', 'meta', 'link', 'button', 'form', 'comment']):\n",
    "        element.decompose()\n",
    "\n",
    "    # Get visible text\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_text = filter(lambda t: t.parent.name not in ['pre', 'code'], texts)\n",
    "    visible_text = filter(lambda x: not isinstance(x, Comment), visible_text)\n",
    "    cleaned_text = ' '.join(t.strip() for t in visible_text if t.strip())\n",
    "    \n",
    "    # Final cleaning\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\[.*?\\]', '', cleaned_text).strip()\n",
    "    \n",
    "    if not cleaned_text:\n",
    "        result['error'] = \"No text content found\"\n",
    "        return result\n",
    "    \n",
    "    result['text'] = cleaned_text\n",
    "\n",
    "    # Perform analysis\n",
    "    try:\n",
    "        # Basic metrics\n",
    "        words = re.findall(r'\\b\\w+\\b', cleaned_text.lower())\n",
    "        sentences = re.split(r'[.!?]+', cleaned_text)\n",
    "        word_count = len(words)\n",
    "        unique_words = len(set(words))\n",
    "        sentence_count = len([s for s in sentences if s.strip()])\n",
    "        \n",
    "        # Readability metrics\n",
    "        avg_word_length = sum(len(word) for word in words) / word_count if word_count > 0 else 0\n",
    "        avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0\n",
    "        \n",
    "        # Keyword analysis\n",
    "        stop_words = {'the', 'and', 'to', 'of', 'a', 'in', 'that', 'is', 'it', 'for', 'on', 'with', 'as', 'was', 'this', 'be', 'by', 'are', 'at', 'from'}\n",
    "        content_words = [word for word in words if word not in stop_words and len(word) > 3]\n",
    "        common_words = Counter(content_words).most_common(5)\n",
    "        \n",
    "        # Metadata\n",
    "        metadata_soup = BeautifulSoup(html, 'html.parser')\n",
    "        title = metadata_soup.title.string.strip() if metadata_soup.title else ''\n",
    "        description = metadata_soup.find('meta', attrs={'name': 'description'})\n",
    "        description = description['content'].strip() if description else ''\n",
    "        \n",
    "        result['analysis'] = {\n",
    "            'metadata': {\n",
    "                'title': title,\n",
    "                'description': description,\n",
    "                'word_count': word_count,\n",
    "                'sentence_count': sentence_count\n",
    "            },\n",
    "            'readability': {\n",
    "                'avg_word_length': round(avg_word_length, 2),\n",
    "                'avg_sentence_length': round(avg_sentence_length, 2),\n",
    "                'unique_word_ratio': round(unique_words / word_count, 3) if word_count > 0 else 0\n",
    "            },\n",
    "            'keywords': [{'word': w[0], 'count': w[1]} for w in common_words]\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        result['error'] = f\"Analysis error: {str(e)}\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    url = input(\"Enter URL to analyze: \")\n",
    "    data = extract_and_analyze(url)\n",
    "    \n",
    "    if data['error']:\n",
    "        print(f\"Error: {data['error']}\")\n",
    "    else:\n",
    "        print(\"\\n=== Extracted Text ===\\n\")\n",
    "        print(data['text'][:1000] + \"...\\n\")\n",
    "        \n",
    "        print(\"=== Analysis ===\")\n",
    "        print(f\"Title: {data['analysis']['metadata']['title']}\")\n",
    "        print(f\"Description: {data['analysis']['metadata']['description']}\")\n",
    "        print(f\"Word Count: {data['analysis']['metadata']['word_count']}\")\n",
    "        print(f\"Sentence Count: {data['analysis']['metadata']['sentence_count']}\")\n",
    "        print(f\"\\nReadability Scores:\")\n",
    "        print(f\"Average Word Length: {data['analysis']['readability']['avg_word_length']}\")\n",
    "        print(f\"Average Sentence Length: {data['analysis']['readability']['avg_sentence_length']}\")\n",
    "        print(f\"Unique Word Ratio: {data['analysis']['readability']['unique_word_ratio']}\")\n",
    "        print(\"\\nTop Keywords:\")\n",
    "        for kw in data['analysis']['keywords']:\n",
    "            print(f\"- {kw['word'].title()} ({kw['count']} occurrences)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0t/rr73mn4s3zx_tw23p5ls40w00000gn/T/ipykernel_2474/4089644211.py:54: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  texts = soup.findAll(text=True)\n",
      "Your max_length is set to 300, but your input_length is only 62. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=31)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Extracted Text (truncated) ===\n",
      "\n",
      "Nasa astronauts Sunita Williams and Butch Wilmore are finally on their way back to Earth via SpaceX , concluding an extraordinary extended mission spanning more than nine months. The vessel detached during the early hours, targeting a water landing near the Florida coastline by early evening, subject to weather conditions. Here's how a journey that was set for 8 days took 9 months What was initially planned as an eight-day mission transformed into a nine-month stay. The original schedule included a June 5 launch using Boeing's Starliner crew capsule. Technical complications during the space station approach led Nasa to return Starliner empty and transfer the test pilots to SpaceX. Further technical issues with the SpaceX capsule caused an additional month's delay. June 2024: Starliner launched for the ISS on June 5, scheduled to return by June 14. However, helium leaks led to multiple delays, pushing the return date first to June 18, then June 26, and beyond. August 2024: Nasa suggeste...\n",
      "\n",
      "\n",
      "=== AI Summary ===\n",
      "\n",
      "Nasa astronauts Sunita Williams and Butch Wilmore are finally on their way back to Earth via SpaceX. The vessel detached during the early hours, targeting a water landing near the Florida coastline by early evening. The extended mission resulted from technical issues with their original return vehicle, Boeing's Starliner capsule. The mission is expected to last until the early morning hours of Monday morning, with a possible landing in the early afternoon if all goes to plan. It is the second mission for Williams and Wilmore.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import readability\n",
    "import re\n",
    "from transformers import pipeline\n",
    "\n",
    "class WebContentSummarizer:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "        }\n",
    "        # Initialize summarization pipeline\n",
    "        self.summarizer = pipeline(\n",
    "            \"summarization\",\n",
    "            model=\"facebook/bart-large-cnn\",\n",
    "            tokenizer=\"facebook/bart-large-cnn\",\n",
    "            framework=\"pt\"\n",
    "        )\n",
    "\n",
    "    def _extract_text(self, url):\n",
    "        \"\"\"Extract and clean text content from webpage\"\"\"\n",
    "        try:\n",
    "            # Validate URL\n",
    "            parsed = urlparse(url)\n",
    "            if not all([parsed.scheme, parsed.netloc]):\n",
    "                return {'error': 'Invalid URL format'}\n",
    "            \n",
    "            # Fetch content\n",
    "            response = requests.get(url, headers=self.headers, timeout=10, verify=True)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            if 'text/html' not in response.headers.get('Content-Type', ''):\n",
    "                return {'error': 'Non-HTML content'}\n",
    "            \n",
    "            html = response.content\n",
    "            \n",
    "            # Extract main content\n",
    "            try:\n",
    "                doc = readability.Document(html)\n",
    "                content_html = doc.summary()\n",
    "            except:\n",
    "                content_html = html\n",
    "\n",
    "            # Clean text\n",
    "            soup = BeautifulSoup(content_html, 'html.parser')\n",
    "            \n",
    "            # Remove unwanted elements\n",
    "            for element in soup(['header', 'footer', 'nav', 'aside', 'script', 'style',\n",
    "                               'noscript', 'meta', 'link', 'button', 'form', 'comment']):\n",
    "                element.decompose()\n",
    "\n",
    "            # Get visible text\n",
    "            texts = soup.findAll(text=True)\n",
    "            visible_text = filter(lambda t: t.parent.name not in ['pre', 'code'], texts)\n",
    "            visible_text = filter(lambda x: not isinstance(x, Comment), visible_text)\n",
    "            cleaned_text = ' '.join(t.strip() for t in visible_text if t.strip())\n",
    "            \n",
    "            # Final cleaning\n",
    "            cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "            cleaned_text = re.sub(r'\\[.*?\\]', '', cleaned_text).strip()\n",
    "            \n",
    "            if not cleaned_text:\n",
    "                return {'error': 'No text content found'}\n",
    "            \n",
    "            return {'text': cleaned_text}\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'error': str(e)}\n",
    "\n",
    "    def _chunk_text(self, text, max_length=1024):\n",
    "        \"\"\"Split text into chunks that fit model's max length\"\"\"\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        \n",
    "        for word in words:\n",
    "            if len(current_chunk) + 1 <= max_length:\n",
    "                current_chunk.append(word)\n",
    "            else:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                current_chunk = [word]\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            \n",
    "        return chunks\n",
    "\n",
    "    def summarize_url(self, url):\n",
    "        \"\"\"Process URL and generate summary\"\"\"\n",
    "        result = {\n",
    "            'url': url,\n",
    "            'extracted_text': '',\n",
    "            'summary': '',\n",
    "            'error': None\n",
    "        }\n",
    "        \n",
    "        # Extract text\n",
    "        extraction = self._extract_text(url)\n",
    "        if 'error' in extraction:\n",
    "            result['error'] = extraction['error']\n",
    "            return result\n",
    "        \n",
    "        text = extraction['text']\n",
    "        result['extracted_text'] = text\n",
    "        \n",
    "        # Generate summary\n",
    "        try:\n",
    "            chunks = self._chunk_text(text)\n",
    "            summaries = []\n",
    "            \n",
    "            for chunk in chunks:\n",
    "                summary = self.summarizer(\n",
    "                    chunk,\n",
    "                    max_length=150,\n",
    "                    min_length=30,\n",
    "                    do_sample=False,\n",
    "                    truncation=True\n",
    "                )\n",
    "                summaries.append(summary[0]['summary_text'])\n",
    "            \n",
    "            full_summary = ' '.join(summaries)\n",
    "            \n",
    "            # Create final concise summary\n",
    "            final_summary = self.summarizer(\n",
    "                full_summary,\n",
    "                max_length=300,\n",
    "                min_length=100,\n",
    "                do_sample=False,\n",
    "                truncation=True\n",
    "            )[0]['summary_text']\n",
    "            \n",
    "            result['summary'] = final_summary\n",
    "            \n",
    "        except Exception as e:\n",
    "            result['error'] = f\"Summarization error: {str(e)}\"\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    summarizer = WebContentSummarizer()\n",
    "    url = input(\"Enter URL to summarize: \")\n",
    "    \n",
    "    result = summarizer.summarize_url(url)\n",
    "    \n",
    "    if result['error']:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "    else:\n",
    "        print(\"\\n=== Extracted Text (truncated) ===\\n\")\n",
    "        print(result['extracted_text'][:1000] + \"...\\n\")\n",
    "        \n",
    "        print(\"\\n=== AI Summary ===\\n\")\n",
    "        print(result['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
